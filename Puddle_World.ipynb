{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using ReinforcementLearning\n",
    "using Flux: InvDecay\n",
    "using Flux.Losses: huber_loss\n",
    "using BSON\n",
    "using StatsBase:mean\n",
    "using Plots\n",
    "using IntervalSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jackpot (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 4\n",
    "l = 4\n",
    "\n",
    "function dist(p1, p2)\n",
    "   ((p1[1] - p2[1])^2 + (p1[2] - p2[2])^2)^0.5\n",
    "end\n",
    "\n",
    "actions = []\n",
    "successful_episode = []\n",
    "\n",
    "function puddle(cur_state, puddle_center, puddle_radius)\n",
    "    (dist(cur_state, puddle_center) < puddle_radius)\n",
    "end\n",
    "\n",
    "function rectangle(cur_state, x, y, width, height)\n",
    "    if (cur_state[1] > x && cur_state[1] < x + width)\n",
    "        if (cur_state[2] > y && cur_state[2] < y + height)\n",
    "            return true\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return false\n",
    "end\n",
    "\n",
    "function jackpot(cur_state, jackpot_center, jackpot_radius)\n",
    "    (dist(cur_state, jackpot_center) < jackpot_radius)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.@kwdef mutable struct PuddleWorld <: AbstractEnv\n",
    "    height::Int = h\n",
    "    length::Int = l\n",
    "    \n",
    "\tcurrent_state::Vector{Float64} = [0, 0]\n",
    "    goal_state::Vector{Float64} = [l-0.5, h-0.5]\n",
    "    goal_radius::Float64 = 0.25\n",
    "    \n",
    "    time::Int = 0\n",
    "    reward::Float64 = 0\n",
    "    \n",
    "    n_actions::Int = 360\n",
    "    episode_num::Int = 0\n",
    "    \n",
    "    j_center1::Vector{Float64} = [1.0, 3.0]\n",
    "    j_center2::Vector{Float64} = [3.0, 1.0]\n",
    "    j_radius::Float64 = 0.25\n",
    "    \n",
    "    visited1::Bool = false\n",
    "    visited2::Bool = false\n",
    "    \n",
    "    move_dist::Float64 = 0.5\n",
    "    \n",
    "    temp::Vector{Int} = []\n",
    "end\n",
    "\n",
    "RLBase.action_space(env::PuddleWorld) = begin\n",
    "\tBase.OneTo(env.n_actions)\n",
    "end\n",
    "\n",
    "#=\n",
    "RLBase.legal_action_space(env::GridWorld) = begin\n",
    "\tcontinuous ? 0..(2.0 * π) : Base.OneTo(n_actions)\n",
    "end\n",
    "\n",
    "RLBase.legal_action_space_mask(env::GridWorld) = begin\n",
    "    map(x -> x∈[1, 2, 3, 4], 1:4)\n",
    "end\n",
    "=#\n",
    "\n",
    "RLBase.state(env::PuddleWorld) = begin \n",
    "\tenv.current_state\n",
    "end\n",
    "\n",
    "RLBase.state_space(env::PuddleWorld) = begin \n",
    "\t Space([0.0..env.length, 0.0..env.height])\n",
    "end\n",
    "\n",
    "RLBase.reward(env::PuddleWorld) = begin \n",
    "    env.reward\n",
    "end\n",
    "\n",
    "RLBase.is_terminated(env::PuddleWorld) = begin\n",
    "\tif env.time > 100\n",
    "        push!(actions, env.temp)\n",
    "        push!(successful_episode, false)\n",
    "        return true\n",
    "    end\n",
    "    \n",
    "    #=\n",
    "    if env.current_state[1] > env.length\n",
    "        return true\n",
    "    elseif env.current_state[1] < 0\n",
    "        return true\n",
    "    elseif env.current_state[2] > env.height\n",
    "        return true\n",
    "    elseif env.current_state[2] < 0\n",
    "        return true\n",
    "    end\n",
    "    =#\n",
    "    \n",
    "    if dist(env.current_state, env.goal_state) < env.goal_radius\n",
    "        push!(actions, env.temp)\n",
    "        push!(successful_episode, true)\n",
    "\t\treturn true\n",
    "\tend\n",
    "    \n",
    "    return false\n",
    "end\n",
    "\n",
    "RLBase.reset!(env::PuddleWorld) = begin\n",
    "\tenv.height = h\n",
    "    env.length = l\n",
    "    env.current_state = [0, 0]\n",
    "    env.time = 0\n",
    "    env.temp = []\n",
    "end\n",
    "\n",
    "function (env::PuddleWorld)(a)\n",
    "    if (env.time == 0)\n",
    "        env.episode_num += 1\n",
    "    end\n",
    "    \n",
    "    fell::Bool = false\n",
    "    \n",
    "    if !is_terminated(env)\n",
    "        push!(env.temp, a)        \n",
    "        env.time += 1\n",
    "\t\t\n",
    "        env.current_state[1] += env.move_dist * cosd(a)\n",
    "        env.current_state[2] += env.move_dist * sind(a)\n",
    "        \n",
    "        if env.current_state[1] > env.length\n",
    "            env.current_state = [0, 0]\n",
    "            fell = true\n",
    "        elseif env.current_state[1] < 0\n",
    "            env.current_state = [0, 0]\n",
    "            fell = true\n",
    "        elseif env.current_state[2] > env.height\n",
    "            env.current_state = [0, 0]\n",
    "            fell = true\n",
    "        elseif env.current_state[2] < 0\n",
    "            env.current_state = [0, 0]\n",
    "            fell = true\n",
    "        end\n",
    "        \n",
    "        \n",
    "        if (fell)\n",
    "            env.reward = -100\n",
    "        else\n",
    "            env.reward = -10\n",
    "        end\n",
    "        \n",
    "        if (jackpot(env.current_state, env.j_center1, env.j_radius) && !env.visited1)\n",
    "            env.reward = 50\n",
    "            env.visited1 = true\n",
    "        elseif (jackpot(env.current_state, env.j_center2, env.j_radius) && !env.visited2)\n",
    "            env.reward = 50\n",
    "            env.visited2 = true\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# PuddleWorld\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..4.0, 0.0..4.0])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(360)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0]\n",
       "```\n"
      ],
      "text/plain": [
       "# PuddleWorld\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..4.0, 0.0..4.0])`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Base.OneTo(360)`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[0.0, 0.0]\n",
       "```\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = PuddleWorld()\n",
    "# hook = TotalRewardPerEpisode()\n",
    "# action_space(env)\n",
    "# run(RandomPolicy(action_space(env)), env, StopAfterEpisode(1_000), hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PuddleWorld"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: The GPU function is being called but the GPU is not accessible. \n",
      "│ Defaulting back to the CPU. (No action is required if you want to run on the CPU).\n",
      "└ @ Flux C:\\Users\\saiko\\.julia\\packages\\Flux\\7nTyc\\src\\functor.jl:187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "├─ policy => typename(QBasedPolicy)\n",
       "│  ├─ learner => typename(DQNLearner)\n",
       "│  │  ├─ approximator => typename(NeuralNetworkApproximator)\n",
       "│  │  │  ├─ model => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×2 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 3\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 360×64 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 360-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ optimizer => typename(ADAM)\n",
       "│  │  │     ├─ eta => 0.001\n",
       "│  │  │     ├─ beta\n",
       "│  │  │     │  ├─ 1\n",
       "│  │  │     │  │  └─ 0.9\n",
       "│  │  │     │  └─ 2\n",
       "│  │  │     │     └─ 0.999\n",
       "│  │  │     ├─ epsilon => 1.0e-8\n",
       "│  │  │     └─ state => typename(IdDict)\n",
       "│  │  ├─ target_approximator => typename(NeuralNetworkApproximator)\n",
       "│  │  │  ├─ model => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×2 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 3\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 360×64 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 360-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ optimizer => typename(ADAM)\n",
       "│  │  │     ├─ eta => 0.001\n",
       "│  │  │     ├─ beta\n",
       "│  │  │     │  ├─ 1\n",
       "│  │  │     │  │  └─ 0.9\n",
       "│  │  │     │  └─ 2\n",
       "│  │  │     │     └─ 0.999\n",
       "│  │  │     ├─ epsilon => 1.0e-8\n",
       "│  │  │     └─ state => typename(IdDict)\n",
       "│  │  ├─ loss_func => typename(typeof(huber_loss))\n",
       "│  │  ├─ min_replay_history => 100\n",
       "│  │  ├─ update_freq => 1\n",
       "│  │  ├─ update_step => 0\n",
       "│  │  ├─ target_update_freq => 100\n",
       "│  │  ├─ sampler => typename(NStepBatchSampler)\n",
       "│  │  │  ├─ γ => 0.99\n",
       "│  │  │  ├─ n => 1\n",
       "│  │  │  ├─ batch_size => 32\n",
       "│  │  │  ├─ stack_size => typename(Nothing)\n",
       "│  │  │  ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│  │  │  └─ cache => typename(Nothing)\n",
       "│  │  ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│  │  ├─ loss => 0.0\n",
       "│  │  └─ is_enable_double_DQN => true\n",
       "│  └─ explorer => typename(EpsilonGreedyExplorer)\n",
       "│     ├─ ϵ_stable => 0.01\n",
       "│     ├─ ϵ_init => 1.0\n",
       "│     ├─ warmup_steps => 0\n",
       "│     ├─ decay_steps => 500\n",
       "│     ├─ step => 1\n",
       "│     ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│     └─ is_training => true\n",
       "└─ trajectory => typename(Trajectory)\n",
       "   └─ traces => typename(NamedTuple)\n",
       "      ├─ state => 2×0 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "      ├─ action => 0-element CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}\n",
       "      ├─ reward => 0-element CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}\n",
       "      └─ terminal => 0-element CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "        policy = QBasedPolicy(\n",
    "            learner = DQNLearner(\n",
    "                approximator = NeuralNetworkApproximator(\n",
    "                    model = Chain(\n",
    "                        Dense(2, 64, relu; init = glorot_uniform()),\n",
    "                        Dense(64, 64, relu; init = glorot_uniform()),\n",
    "                        Dense(64, 360; init = glorot_uniform()),\n",
    "                    ) |> gpu,\n",
    "                    optimizer = ADAM(),\n",
    "                ),\n",
    "                target_approximator = NeuralNetworkApproximator(\n",
    "                    model = Chain(\n",
    "                        Dense(2, 64, relu; init = glorot_uniform()),\n",
    "                        Dense(64, 64, relu; init = glorot_uniform()),\n",
    "                        Dense(64, 360; init = glorot_uniform()),\n",
    "                    ) |> gpu,\n",
    "                    optimizer = ADAM(),\n",
    "                ),\n",
    "                loss_func = huber_loss,\n",
    "                stack_size = nothing,\n",
    "                batch_size = 32,\n",
    "                update_horizon = 1,\n",
    "                min_replay_history = 100,\n",
    "                update_freq = 1,\n",
    "                target_update_freq = 100,\n",
    "            ),\n",
    "            explorer = EpsilonGreedyExplorer(\n",
    "                kind = :exp,\n",
    "                ϵ_stable = 0.01,\n",
    "                decay_steps = 500,\n",
    "            ),\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 500,\n",
    "            state = Vector{Float32} => (2,),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  17%|███████                                  |  ETA: 0:06:04\u001b[39m"
     ]
    }
   ],
   "source": [
    "hookAgent = TotalRewardPerEpisode(is_display_on_exit = false)\n",
    "run(agent, env, StopAfterEpisode(1000),hookAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hookAgent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookRand = TotalRewardPerEpisode(is_display_on_exit = false)\n",
    "run(RandomPolicy(action_space(env)), env, StopAfterEpisode(1000), hookRand)\n",
    "plot(hookRand.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figurated learning\n",
    "# distributed network\n",
    "# gains with physics are higher in larger systems\n",
    "# mixed precision - faster process with no loss of quality\n",
    "# annealing\n",
    "# - higher noise initially to prevent you from going into local minimums\n",
    "# - lower noise at the end when you are closer to optimal policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy.explorer.is_training = false\n",
    "\n",
    "hookAgent = TotalRewardPerEpisode(is_display_on_exit = false)\n",
    "run(agent, env, StopAfterEpisode(1000),hookAgent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hookAgent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in length(successful_episode):-1:1\n",
    "    if (successful_episode[i])\n",
    "        for k in actions[i]\n",
    "            println(k)\n",
    "        end\n",
    "        \n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total = 0\n",
    "\n",
    "for i in length(successful_episode):-1:length(successful_episode)-999\n",
    "    if (successful_episode[i] == true)\n",
    "        counter += 1\n",
    "    end\n",
    "    \n",
    "    total += 1\n",
    "end\n",
    "\n",
    "print(\"Total: \")\n",
    "println(total)\n",
    "print(\"Successful: \")\n",
    "println(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total = 0\n",
    "\n",
    "for i in 1:1:1000\n",
    "    if (successful_episode[i] == true)\n",
    "        counter += 1\n",
    "    end\n",
    "    \n",
    "    total += 1\n",
    "end\n",
    "\n",
    "print(\"Total: \")\n",
    "println(total)\n",
    "print(\"Successful: \")\n",
    "println(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total = 0\n",
    "\n",
    "for i in 1:1:length(successful_episode)\n",
    "    if (successful_episode[i] == true)\n",
    "        counter += 1\n",
    "    end\n",
    "    \n",
    "    total += 1\n",
    "end\n",
    "\n",
    "print(\"Total: \")\n",
    "println(total)\n",
    "print(\"Successful: \")\n",
    "println(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(length(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
